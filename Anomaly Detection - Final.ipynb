{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152c8828",
   "metadata": {},
   "source": [
    "# Network Anomaly Detection using Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9dcf3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import random\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b800",
   "metadata": {},
   "source": [
    "# 1. Importing Data and Understanding Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4ac7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.data_preprocessing import preprocess_data_10, preprocess_data, category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "36eab8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_means, labels_kmeans = preprocess_data_10()\n",
    "data_spectral, labels_spectral = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98946dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter, OrderedDict\n",
    "# d = Counter(labels_spectral)\n",
    "# OrderedDict(sorted(d.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0051450",
   "metadata": {},
   "source": [
    "# 2.  Clustering Using K-Means and Normalized Cut (Your implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b39528",
   "metadata": {},
   "source": [
    "### K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06553527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes two attrs k number of centroids and the whole data set number of samples x features\n",
    "\n",
    "def kMeans_implemented(k,data):\n",
    "    centroids=[]\n",
    "    num_points=data.shape[0]\n",
    "    num_features=data.shape[1]\n",
    "    \n",
    "    #Appending random points to be our centroids according to the number of ks\n",
    "    for i in range(k):\n",
    "        centroids.append(data[random.randint(0, num_points)])\n",
    "    clusters={}\n",
    "    t=0\n",
    "    while(True):\n",
    "        labels=[]\n",
    "        #Initialize empty clusters\n",
    "        for i in range (k):\n",
    "            clusters[i]=[]\n",
    "            \n",
    "        #Classify the points according to the closest centroid\n",
    "        for i in range(num_points):\n",
    "            distances=[]\n",
    "            for j in range(k):\n",
    "                distances.append(np.linalg.norm(data[i]-centroids[j]))\n",
    "            clusters[distances.index(min(distances))].append(data[i])\n",
    "            labels.append(distances.index(min(distances)))\n",
    "        new_centroids=np.zeros((k,num_features))\n",
    "        \n",
    "        #Measuring the new centroids\n",
    "        for i in range(k):\n",
    "            new_centroids[i]=np.mean(clusters[i],axis=0)\n",
    "        if(centroids==new_centroids).all():\n",
    "            break\n",
    "        else:\n",
    "            centroids=new_centroids\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225e45c",
   "metadata": {},
   "source": [
    "### Spectral Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac6f2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def spectral_clustering(A,k):\n",
    "        \n",
    "    #--------------computing the degree matrix-------------\n",
    "    d = np.diag(np.sum(A, axis=1))\n",
    "\n",
    "    #--------------------computing L-----------------------\n",
    "    L = d-A\n",
    "\n",
    "    #---------------------computing La---------------------\n",
    "    #computing the inverse of the dgree matrix\n",
    "    inv_degree = np.linalg.inv(d)\n",
    "    La = np.dot(inv_degree, L)\n",
    "\n",
    "    #---computing the eigenValues and eigenVectors of La---\n",
    "    e_val, evec = np.linalg.eig(La)\n",
    "\n",
    "    #----------sorting the eigenValues ascending----------- \n",
    "    idx = np.argsort(eval)\n",
    "    e_val = e_val[idx]\n",
    "\n",
    "    #---sorting the eigenVectors according to their corresponding eigenValues---\n",
    "    evec = evec[:, idx]\n",
    "\n",
    "    #--slicing the eigenVectors to the desired number of clusters--\n",
    "    evec_new = evec[:, :k]\n",
    "\n",
    "    #-------------normalizing the eigenVectors--------------\n",
    "    system = evec.real / np.sqrt(np.linalg.norm(evec.real))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    system_labels = kmeans.fit_predict(system)\n",
    "\n",
    "\n",
    "    return system, system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc29afd",
   "metadata": {},
   "source": [
    "## GMM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac20cc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, k, max_iter=5):\n",
    "        self.k = k\n",
    "        self.max_iter = int(max_iter)\n",
    "\n",
    "    def initialize(self, X):\n",
    "        self.shape = X.shape\n",
    "        self.n, self.m = self.shape\n",
    "\n",
    "        self.phi = np.full(shape=self.k, fill_value=1/self.k)\n",
    "        self.weights = np.full( shape=self.shape, fill_value=1/self.k)\n",
    "        \n",
    "        random_row = np.random.randint(low=0, high=self.n, size=self.k)\n",
    "        self.mu = [  X[row_index,:] for row_index in random_row ]\n",
    "        self.sigma = [ np.cov(X.T) for _ in range(self.k) ]\n",
    "\n",
    "    def e_step(self, X):\n",
    "        # E-Step: update weights and phi holding mu and sigma constant\n",
    "        self.weights = self.predict_proba(X)\n",
    "        self.phi = self.weights.mean(axis=0)\n",
    "    \n",
    "    def m_step(self, X):\n",
    "        # M-Step: update mu and sigma holding phi and weights constant\n",
    "        for i in range(self.k):\n",
    "            weight = self.weights[:, [i]]\n",
    "            total_weight = weight.sum()\n",
    "            self.mu[i] = (X * weight).sum(axis=0) / total_weight\n",
    "            self.sigma[i] = np.cov(X.T, \n",
    "                aweights=(weight/total_weight).flatten(), \n",
    "                bias=True)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.initialize(X)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            self.e_step(X)\n",
    "            self.m_step(X)\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        likelihood = np.zeros( (self.n, self.k) )\n",
    "        for i in range(self.k):\n",
    "            distribution = multivariate_normal(\n",
    "                mean=self.mu[i], \n",
    "                cov=self.sigma[i],\n",
    "                allow_singular=True\n",
    "            )\n",
    "            likelihood[:,i] = distribution.pdf(X)\n",
    "        \n",
    "        numerator = likelihood * self.phi\n",
    "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
    "        weights = numerator / denominator\n",
    "        return weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        weights = self.predict_proba(X)\n",
    "        return np.argmax(weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "145e436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, n_components=1, max_iter=100, tol=1e-4):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.gmm = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.gmm = GaussianMixture(n_components=self.n_components, max_iter=self.max_iter, tol=self.tol)\n",
    "        self.gmm.fit(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.gmm.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.gmm.predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45d2c1",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dead2d3",
   "metadata": {},
   "source": [
    "## K-Means Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af891c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = kMeans_implemented(7,np.array(data_k_means))\n",
    "labels2 = kMeans_implemented(15,np.array(data_k_means))\n",
    "labels3 = kMeans_implemented(23,np.array(data_k_means))\n",
    "labels4 = kMeans_implemented(31,np.array(data_k_means))\n",
    "labels5 = kMeans_implemented(45,np.array(data_k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6762ea48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.35235793694748563\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.63, 0.93, 0.99, 1.0, 0.98, 0.72, 0.83]\n",
      "Purity: 0.9272251452749578\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.189, 0.0892, 0.3665, 0.8109, 0.362, 0.0454, 0.0839]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.37865756371357984\n"
     ]
    }
   ],
   "source": [
    "#labels = kMeans_implemented(7,np.array(data_k_means))\n",
    "contingency_matrix = get_contingency(labels1,np.array(labels_kmeans))\n",
    "evaluation(np.array(data_k_means),contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5271b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.19353415547078842\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.97, 0.95, 0.97, 0.98, 0.7, 0.99, 0.98, 0.98, 0.91, 0.97, 1.0, 0.53, 1.0, 0.57, 1.0]\n",
      "Purity: 0.9672770733449645\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.0583, 0.0446, 0.0531, 0.0524, 0.7551, 0.0723, 0.2908, 0.294, 0.189, 0.1071, 0.3826, 0.0045, 0.9913, 0.0203, 0.4283]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.324018694422339\n"
     ]
    }
   ],
   "source": [
    "#labels = kMeans_implemented(15,np.array(data_k_means))\n",
    "contingency_matrix = get_contingency(labels2,np.array(labels_kmeans))\n",
    "evaluation(np.array(data_k_means),contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "609a13cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.2619780402966085\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.98, 1.0, 1.0, 0.67, 0.93, 0.96, 1.0, 0.98, 0.94, 0.97, 1.0, 1.0, 0.67, 1.0, 1.0, 0.54, 0.95, 0.74, 0.98, 0.7, 0.98, 0.69, 1.0]\n",
      "Purity: 0.9368002417814901\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.252, 0.0557, 0.2481, 0.189, 0.7859, 0.0411, 0.1737, 0.1007, 0.0203, 0.1515, 0.0676, 0.1846, 0.8356, 0.1782, 0.0567, 0.0042, 0.0052, 0.0348, 0.0222, 0.7582, 0.0259, 0.0252, 0.106]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.245611790960462\n"
     ]
    }
   ],
   "source": [
    "#labels = kMeans_implemented(23,np.array(data_k_means))\n",
    "contingency_matrix = get_contingency(labels3,np.array(labels_kmeans))\n",
    "evaluation(data_k_means,contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06166a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.14174278422676723\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [1.0, 1.0, 1.0, 0.95, 0.99, 0.65, 1.0, 1.0, 1.0, 0.98, 1.0, 0.98, 1.0, 0.9, 0.95, 0.99, 0.96, 0.87, 0.95, 1.0, 1.0, 0.99, 1.0, 1.0, 0.97, 0.95, 1.0, 0.69, 0.54, 0.93, 0.71]\n",
      "Purity: 0.9756638687785916\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.0024, 0.1489, 0.1019, 0.0565, 0.0789, 0.75, 0.0295, 0.0458, 0.9913, 0.005, 0.1252, 0.2148, 0.1567, 0.0042, 0.0174, 0.0289, 0.037, 0.0137, 0.624, 0.0777, 0.0585, 0.0528, 0.0001, 0.1569, 0.189, 0.7826, 0.3062, 0.8356, 0.0042, 0.0624, 0.0253]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.24337134966674756\n"
     ]
    }
   ],
   "source": [
    "#labels = kMeans_implemented(31,np.array(data_k_means))\n",
    "contingency_matrix = get_contingency(labels4,np.array(labels_kmeans))\n",
    "evaluation(np.array(data_k_means),contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63ed3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.1250806575035399\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.96, 0.68, 1.0, 0.95, 1.0, 0.79, 1.0, 0.99, 0.95, 0.9, 1.0, 0.99, 1.0, 0.85, 0.87, 0.98, 0.96, 1.0, 1.0, 1.0, 0.74, 0.54, 0.96, 1.0, 0.47, 1.0, 1.0, 0.98, 1.0, 1.0, 0.99, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 1.0, 0.72, 1.0, 1.0, 0.69, 1.0, 0.97, 1.0]\n",
      "Purity: 0.9765155990273789\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.0368, 0.0041, 0.0288, 0.624, 0.0533, 0.8326, 0.7362, 0.226, 0.0348, 0.0042, 0.0859, 0.0234, 0.0422, 0.0049, 0.0136, 0.0151, 0.0497, 0.0987, 0.1554, 0.1554, 0.0151, 0.0042, 0.0969, 0.0921, 0.0004, 0.0562, 0.0244, 0.0127, 0.0585, 0.0401, 0.0668, 0.0295, 0.0946, 0.005, 0.0135, 0.0316, 0.0047, 0.125, 0.0213, 0.0271, 0.9913, 0.2885, 0.0307, 0.016, 0.0997]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.1657574004027622\n"
     ]
    }
   ],
   "source": [
    "#labels = kMeans_implemented(45,np.array(data_k_means))\n",
    "contingency_matrix = get_contingency(labels5,np.array(labels_kmeans))\n",
    "evaluation(np.array(data_k_means),contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3df0d",
   "metadata": {},
   "source": [
    "## Spectral Clustering Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33d12a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to use it in spectral clustering\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_spectral, labels_spectral, test_size=0.995, train_size=0.005,stratify=labels_spectral,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b9ad3cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune.\n",
      "normal.\n",
      "teardrop.\n",
      "ipsweep.\n",
      "back.\n",
      "satan.\n",
      "smurf.\n",
      "portsweep.\n",
      "pod.\n",
      "nmap.\n",
      "warezclient.\n"
     ]
    }
   ],
   "source": [
    "# Explore the classes in training dataset after splitting\n",
    "for i in y_train.unique():\n",
    "    print(category_map(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0df13c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix=rbf_kernel(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e6425e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system,labels_spectral=spectral_clustering(sim_matrix,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3abd7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_y={}\n",
    "y=list(y_train.unique())\n",
    "for i in range (len(y)):\n",
    "    dict_y[y_train.unique()[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ef6152af",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_train=np.array([dict_y[label] for label in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87d4e96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.5290454824191261\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.66, 0.6]\n",
      "Purity: 0.8262002232973576\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.0347, 0.2, 0.2, 0.2, 0.2, 0.2, 1.0, 0.0015, 0.7128, 0.0861, 0.8596]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.4024869886301618\n"
     ]
    }
   ],
   "source": [
    "contingency_matrix = get_contingency(labels_spectral,new_y_train)\n",
    "evaluation(X_train,contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed715399",
   "metadata": {},
   "source": [
    "## Comparison between K-means and Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3ec9fe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.17327577432161273\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [1.0, 0.52, 1.0, 0.72, 1.0, 1.0, 0.99, 0.98, 1.0, 0.9, 0.55]\n",
      "Purity: 0.9518049869743208\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.351, 0.0482, 0.0081, 0.0135, 0.0827, 0.0047, 0.3281, 0.1329, 0.8629, 0.0281, 0.9474]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.29239279534715756\n"
     ]
    }
   ],
   "source": [
    "labels6 = kMeans_implemented(11,np.array(X_train))\n",
    "contingency_matrix = get_contingency(labels6,new_y_train)\n",
    "evaluation(X_train,contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3784e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 14, 7: 11, 8: 11, 9: 11, 10: 9}\n"
     ]
    }
   ],
   "source": [
    "new_labels_spectral,mapped_spectral = map_and_change(y_train,labels_spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f331514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_spectral=cluster_to_class_name(new_labels_spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7e050133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'normal.',\n",
       " 1: 'teardrop.',\n",
       " 2: 'teardrop.',\n",
       " 3: 'teardrop.',\n",
       " 4: 'teardrop.',\n",
       " 5: 'teardrop.',\n",
       " 6: 'pod.',\n",
       " 7: 'normal.',\n",
       " 8: 'normal.',\n",
       " 9: 'normal.',\n",
       " 10: 'neptune.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2becc5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11, 1: 11, 2: 11, 3: 11, 4: 11, 5: 11, 6: 11, 7: 11, 8: 9, 9: 11, 10: 5}\n"
     ]
    }
   ],
   "source": [
    "new_labels_kmeans,mapped_kmeans = map_and_change(y_train,labels6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "612c8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_kmeans=cluster_to_class_name(new_labels_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c32c7f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'normal.',\n",
       " 1: 'normal.',\n",
       " 2: 'normal.',\n",
       " 3: 'normal.',\n",
       " 4: 'normal.',\n",
       " 5: 'normal.',\n",
       " 6: 'normal.',\n",
       " 7: 'normal.',\n",
       " 8: 'neptune.',\n",
       " 9: 'normal.',\n",
       " 10: 'ipsweep.'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16bb0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_spectral,abnormal_spectral=classify_normality(mapped_spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b45f9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_kmeans,abnormal_kmeans=classify_normality(mapped_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8fe9f86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of normal samples using spectral clustering= 3637\n",
      "Number of abnormal samples using spectral clustering= 1737\n",
      "Number of normal samples using kmeans clustering= 4296\n",
      "Number of abnormal samples using kmeans clustering= 1078\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of normal samples using spectral clustering= {normal_spectral}\")\n",
    "print(f\"Number of abnormal samples using spectral clustering= {abnormal_spectral}\")\n",
    "print(f\"Number of normal samples using kmeans clustering= {normal_kmeans}\")\n",
    "print(f\"Number of abnormal samples using kmeans clustering= {abnormal_kmeans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843c3ef",
   "metadata": {},
   "source": [
    "## GMM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a35f00f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm = GMM(23)\n",
    "labels = gmm.fit(np.array(data_k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf4b6be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = gmm.predict(np.array(data_k_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "004b4cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Conditional Entropy--------------------\n",
      "Conditional Entropy: 0.17864866395510653\n",
      "----------------Purity---------------------------\n",
      "Per cluster purity: [0.6, 1.0, 1.0, 1.0, 0.54, 0.76, 0.95, 0.76, 1.0, 1.0, 0.71, 0.87, 0.89, 1.0, 0.5, 1.0, 0.91, 0.91, 0.85, 0.89, 0.45, 0.97, 0.4]\n",
      "Purity: 0.956383168711277\n",
      "--------------Recalls---------------------------\n",
      "Per cluster Recall: [0.0158, 0.7774, 0.0024, 0.1838, 0.0042, 0.0118, 1.0, 0.0346, 0.0, 0.6992, 0.75, 0.0539, 0.066, 0.9913, 0.125, 0.0001, 0.0001, 0.0005, 0.9811, 0.0622, 0.0233, 0.0445, 0.6]\n",
      "--------------F-measure---------------------------\n",
      "F: 0.30014664845251915\n"
     ]
    }
   ],
   "source": [
    "contingency_matrix = get_contingency(labels,labels_kmeans)\n",
    "evaluation(np.array(data_k_means),contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a443b7d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ac1d73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_to_class_name(labels_dict):\n",
    "    for key in labels_dict.keys():\n",
    "        value=labels_dict[key]\n",
    "        labels_dict[key]=category_map(value)\n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de4c97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_normality(mapped_labels):\n",
    "    normal=0\n",
    "    abnormal=0\n",
    "    for label in mapped_labels:\n",
    "        if label==11:\n",
    "            normal+=1\n",
    "        else:\n",
    "            abnormal+=1\n",
    "    return normal,abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60d26bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels resulting in k-means to true labels in able to do predictions\n",
    "def map_and_change(y_train, labels):\n",
    "    mapping = {}\n",
    "    labels = np.array(list(labels))\n",
    "    for i in np.unique(labels):\n",
    "        binary = [int(x) for x in labels == i]\n",
    "        mapping[i] = np.bincount([value for value, flag in zip(y_train, binary) if flag == 1]).argmax()\n",
    "\n",
    "    # Map the cluster labels to the true class labels\n",
    "    mapped_labels = np.array([mapping[label] for label in labels])\n",
    "\n",
    "    # Print the mapped labels\n",
    "    print(mapping)\n",
    "    return mapping, mapped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d24166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_change_test(mapping, labels):\n",
    "    mapped_labels = np.array([mapping[label] for label in labels])\n",
    "    return mapped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9fa25635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contingency(labels,true):\n",
    "    labels = list(labels)\n",
    "    true = list(true)\n",
    "    true_len=np.unique(np.array(true)).shape[0]\n",
    "    cluster_len=np.unique(np.array(labels)).shape[0]\n",
    "    num_elements = len(labels)\n",
    "    contingency_matrix = np.zeros((true_len,cluster_len))\n",
    "    for i in range(num_elements):\n",
    "        contingency_matrix[true[i],labels[i]] += 1\n",
    "    return contingency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e07ec46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data, contingency_matrix):\n",
    "    n_total = data.shape[0]\n",
    "    gt_classes=contingency_matrix.shape[0]\n",
    "    predicted_classes=contingency_matrix.shape[1]\n",
    "#     TP, TN, FP, FN = 0, 0, 0, 0\n",
    "#     # True Positive \n",
    "#     for i in range(gt_classes):\n",
    "#         for j in range(predicted_classes):\n",
    "#             if contingency_matrix[i][j] != 1 and contingency_matrix[i][j] != 0:\n",
    "#                 TP += math.comb(int(contingency_matrix[i][j]),2)\n",
    "\n",
    "#     # True Negative \n",
    "#     for i in range(gt_classes):\n",
    "#         for j in range(predicted_classes):\n",
    "#             if i != j:\n",
    "#                 for k in range(predicted_classes):\n",
    "#                     temp = contingency_matrix[k,i]*(np.sum(contingency_matrix[:,j]) - contingency_matrix[k,j])\n",
    "#                     TN += temp\n",
    "#     TN = TN/2\n",
    "\n",
    "#     # False Positive \n",
    "#     for i in range(gt_classes):\n",
    "#         for j in range(predicted_classes):\n",
    "#             temp = contingency_matrix[j,i]*(np.sum(contingency_matrix[:,i])-contingency_matrix[j,i])/2\n",
    "#             FP += temp\n",
    "\n",
    "#     # False Negative \n",
    "#     for i in range(gt_classes):\n",
    "#         for j in range(predicted_classes):\n",
    "#             if i != j:\n",
    "#                 for k in range(predicted_classes):\n",
    "#                     temp = contingency_matrix[k,i]*(contingency_matrix[k,j])\n",
    "#                     FN += temp\n",
    "#     FN /= 2\n",
    "#     print(f\"Fake purity= {(TP)/(TP+FP)}\")\n",
    "#     print(f\"Fake recall= {(TP)/(TP+FN)}\")\n",
    "#     # Jaccard Index\n",
    "#     jacc = TP / (TP + FN + FP)\n",
    "\n",
    "#     # Rand Index\n",
    "#     rand = (TP + TN)/ (TP + FN + FP + TN)\n",
    "#     print('---------Confusion Matrix----------------------')\n",
    "#     print(f\"Rand Index: {rand}\")\n",
    "    \n",
    "#     print(f\"Jaccard Index: {jacc}\")\n",
    "#     print(f'TP= {TP},TN= {TN},FN= {FN},FP= {FP}')\n",
    "    \n",
    "    ht_c = 0\n",
    "    for i in range(predicted_classes):\n",
    "        cluster_elem = np.sum(contingency_matrix[:,i])\n",
    "        for j in range(gt_classes):  \n",
    "            temp = contingency_matrix[j][i]/cluster_elem\n",
    "            if temp != 0:\n",
    "                ht_c += temp*math.log(temp,2)*(cluster_elem/n_total)\n",
    "    ht_c = -1*ht_c\n",
    "    print('---------Conditional Entropy--------------------')\n",
    "    print(f\"Conditional Entropy: {ht_c}\")\n",
    "    \n",
    "    print('----------------Purity---------------------------')\n",
    "    purity=0\n",
    "    purities=[]\n",
    "    recalls=[]\n",
    "    for i in range(predicted_classes):\n",
    "        cluster_sum=np.sum(contingency_matrix[:,i])\n",
    "        class_max=np.max(contingency_matrix[:,i])\n",
    "        a=contingency_matrix[:,i]\n",
    "        max_index=a.argmax()\n",
    "        purities.append(round((class_max/cluster_sum),2))\n",
    "        recalls.append(round((class_max/np.sum(contingency_matrix[max_index,:])),4))\n",
    "        purity+=(class_max/cluster_sum) * (cluster_sum/n_total)\n",
    "    #purity = np.sum(np.max(contingency_matrix, axis =0))/np.sum(contingency_matrix)\n",
    "    print(f\"Per cluster purity: {purities}\")\n",
    "    print(f\"Purity: {purity}\")\n",
    "    print('--------------Recalls---------------------------')\n",
    "    print(f\"Per cluster Recall: {recalls}\")\n",
    "    print('--------------F-measure---------------------------')\n",
    "    # a row for each cluster, and columns are precision, recall and F-measure respectively\n",
    "    \n",
    "    f_measure=0\n",
    "    for i in range(predicted_classes):\n",
    "        f_measure+=(2*purities[i]*recalls[i])/(purities[i]+recalls[i])\n",
    "    f_measure=f_measure/predicted_classes\n",
    "    print(f\"F: {f_measure}\")\n",
    "    \n",
    "#     print('--------------Max matching------------------------')\n",
    "#     row_ind, col_ind = linear_sum_assignment(contingency_matrix, maximize=True)\n",
    "#     contingency_reordered = contingency_matrix[row_ind][:, col_ind]\n",
    "#     #print(contingency_reordered)\n",
    "#     max_match = np.sum(np.diag(contingency_reordered))/np.sum(contingency_matrix)\n",
    "#     print(f\"Max Matching: {max_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix_metrics(contingency_matrix):\n",
    "    # Compute the number of true labels and the number of cluster labels\n",
    "    num_true_labels, num_cluster_labels = contingency_matrix.shape\n",
    "\n",
    "    # Compute the total number of samples\n",
    "    N = contingency_matrix.sum()\n",
    "\n",
    "    # Compute the true positives (samples correctly assigned to the same cluster as their true label)\n",
    "    TP = sum([contingency_matrix[i, j] * (contingency_matrix[i, j] - 1) / 2 for i in range(num_true_labels) for j in range(num_cluster_labels)])\n",
    "\n",
    "    # Compute the false negatives (samples incorrectly assigned to a different cluster than their true label)\n",
    "    FN = sum([contingency_matrix[i, j] * (contingency_matrix[i, :].sum() - contingency_matrix[i, j]) for i in range(num_true_labels) for j in range(num_cluster_labels)])\n",
    "\n",
    "    # Compute the false positives (samples assigned to the same cluster as another true label)\n",
    "    FP = sum([contingency_matrix[i, j] * (contingency_matrix[:, j].sum() - contingency_matrix[i, j]) for i in range(num_true_labels) for j in range(num_cluster_labels)])\n",
    "\n",
    "    # Compute the true negatives (samples correctly assigned to a different cluster than any true label)\n",
    "    TN = N * (N - 1) / 2 - TP - FN - FP\n",
    "\n",
    "    # Return the TP, FN, FP, and TN\n",
    "    return TP, FN, FP, TN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4b8fe",
   "metadata": {},
   "source": [
    "# Testing K-means using Test Data set and Mapping Clusters to Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "502644a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1074992, 5374]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_spectral\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_spectral\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_spectral\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1074992, 5374]"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_spectral, labels_spectral,test_size=0.8,train_size=0.2,stratify=labels_spectral,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a52ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=23, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26774a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b402396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping, train_labels = map_and_change(y_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5091199",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = map_and_change_test(mapping,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_labels, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7435e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
