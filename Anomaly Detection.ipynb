{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152c8828",
   "metadata": {},
   "source": [
    "# Network Anomaly Detection using Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9dcf3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import random\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b800",
   "metadata": {},
   "source": [
    "# 1. Importing Data and Understanding Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4ac7df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.data_preprocessing import preprocess_data_10, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36eab8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_means, labels_kmeans = preprocess_data_10()\n",
    "data_spectral, labels_spectral = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0051450",
   "metadata": {},
   "source": [
    "# 2.  Clustering Using K-Means and Normalized Cut (Your implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b39528",
   "metadata": {},
   "source": [
    "### K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06553527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes two attrs k number of centroids and the whole data set number of samples x features\n",
    "\n",
    "def kMeans_implemented(k,data):\n",
    "    centroids=[]\n",
    "    num_points=data.shape[0]\n",
    "    num_features=data.shape[1]\n",
    "    \n",
    "    #Appending random points to be our centroids according to the number of ks\n",
    "    for i in range(k):\n",
    "        centroids.append(data[random.randint(0, num_points)])\n",
    "    clusters={}\n",
    "    t=0\n",
    "    while(True):\n",
    "        labels=[]\n",
    "        #Initialize empty clusters\n",
    "        for i in range (k):\n",
    "            clusters[i]=[]\n",
    "            \n",
    "        #Classify the points according to the closest centroid\n",
    "        for i in range(num_points):\n",
    "            distances=[]\n",
    "            for j in range(k):\n",
    "                distances.append(np.linalg.norm(data[i]-centroids[j]))\n",
    "            clusters[distances.index(min(distances))].append(data[i])\n",
    "            labels.append(distances.index(min(distances)))\n",
    "        new_centroids=np.zeros((k,num_features))\n",
    "        \n",
    "        #Measuring the new centroids\n",
    "        for i in range(k):\n",
    "            new_centroids[i]=np.mean(clusters[i],axis=0)\n",
    "        if(centroids==new_centroids).all():\n",
    "            break\n",
    "        else:\n",
    "            centroids=new_centroids\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225e45c",
   "metadata": {},
   "source": [
    "### Spectral Clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac6f2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def spectral_clustering(A,k):\n",
    "        \n",
    "    #--------------computing the degree matrix-------------\n",
    "    d = np.diag(np.sum(A, axis=1))\n",
    "\n",
    "    #--------------------computing L-----------------------\n",
    "    L = d-A\n",
    "\n",
    "    #---------------------computing La---------------------\n",
    "    #computing the inverse of the dgree matrix\n",
    "    inv_degree = np.linalg.inv(d)\n",
    "    La = np.dot(inv_degree, L)\n",
    "\n",
    "    #---computing the eigenValues and eigenVectors of La---\n",
    "    e_val, evec = np.linalg.eig(La)\n",
    "\n",
    "    #----------sorting the eigenValues ascending----------- \n",
    "    idx = np.argsort(eval)\n",
    "    e_val = e_val[idx]\n",
    "\n",
    "    #---sorting the eigenVectors according to their corresponding eigenValues---\n",
    "    evec = evec[:, idx]\n",
    "\n",
    "    #--slicing the eigenVectors to the desired number of clusters--\n",
    "    evec_new = evec[:, :k]\n",
    "\n",
    "    #-------------normalizing the eigenVectors--------------\n",
    "    system = evec.real / np.sqrt(np.linalg.norm(evec.real))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    system_labels = kmeans.fit_predict(system)\n",
    "\n",
    "\n",
    "    return system, system_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29bb14",
   "metadata": {},
   "source": [
    "## DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "a015485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "def dbscan(X, eps=0.5, min_samples=10):\n",
    "    \"\"\"\n",
    "    DBSCAN clustering algorithm implementation from scratch.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- numpy array of shape (n_samples, n_features), representing the dataset to be clustered\n",
    "    eps -- float, the maximum distance between two points in the same neighborhood (default=0.5)\n",
    "    min_samples -- int, the minimum number of points required to form a dense region (default=5)\n",
    "    \n",
    "    Returns:\n",
    "    labels -- numpy array of shape (n_samples,), representing the cluster assignments of each data point\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the list of core points and the array of cluster labels\n",
    "    core_points = []\n",
    "    labels = np.full(X.shape[0], -1)\n",
    "    cluster_id = 0\n",
    "    \n",
    "    # compute the pairwise distances between data points\n",
    "    distances = pairwise_distances(X)\n",
    "    \n",
    "    # identify the core points and their neighborhoods\n",
    "    for i in range(X.shape[0]):\n",
    "        neighborhood = np.where(distances[i] <= eps)[0]\n",
    "        if neighborhood.shape[0] >= min_samples:\n",
    "            core_points.append(i)\n",
    "            labels[i] = cluster_id\n",
    "            for j in neighborhood:\n",
    "                if labels[j] == -1:\n",
    "                    labels[j] = cluster_id\n",
    "                    \n",
    "            # grow the cluster\n",
    "            while True:\n",
    "                old_size = np.sum(labels == cluster_id)\n",
    "                for j in core_points:\n",
    "                    neighborhood = np.where(distances[j] <= eps)[0]\n",
    "                    for k in neighborhood:\n",
    "                        if labels[k] == -1:\n",
    "                            labels[k] = cluster_id\n",
    "                            core_points.append(k)\n",
    "                new_size = np.sum(labels == cluster_id)\n",
    "                if old_size == new_size:\n",
    "                    break\n",
    "            \n",
    "            # move to the next cluster\n",
    "            cluster_id += 1\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "33d12a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to use it in spectral clustering\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_spectral, labels_spectral, test_size=0.995, train_size=0.005,stratify=labels_spectral,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0df13c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix=rbf_kernel(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "1e6425e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system,labels=spectral_clustering(sim_matrix,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "cd6e9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbscan(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "190b6d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1       1620\n",
       " 3146      10\n",
       " 1031       7\n",
       " 705        7\n",
       " 613        6\n",
       "         ... \n",
       " 1231       1\n",
       " 1232       1\n",
       " 1233       1\n",
       " 1234       1\n",
       " 3561       1\n",
       "Length: 3563, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.DataFrame(labels)\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a443b7d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "60d26bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels resulting in k-means to true labels in able to do predictions\n",
    "def map_and_change(y_train, labels):\n",
    "    mapping = {}\n",
    "    labels = np.array(list(labels))\n",
    "    for i in np.unique(labels):\n",
    "        binary = [int(x) for x in labels == i]\n",
    "        mapping[i] = np.bincount([value for value, flag in zip(y_train, binary) if flag == 1]).argmax()\n",
    "\n",
    "    # Map the cluster labels to the true class labels\n",
    "    mapped_labels = np.array([mapping[label] for label in labels])\n",
    "\n",
    "    # Print the mapped labels\n",
    "    print(mapping)\n",
    "    return mapping, mapped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4d24166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_change_test(mapping, labels):\n",
    "    mapped_labels = np.array([mapping[label] for label in labels])\n",
    "    return mapped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "021a29a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 14, 7: 11, 8: 11, 9: 11, 10: 11, 11: 21, 12: 11, 13: 11, 14: 11, 15: 11, 16: 11, 17: 9, 18: 18, 19: 11, 20: 11, 21: 11, 22: 11}\n"
     ]
    }
   ],
   "source": [
    "new_labels = map_and_change(y_train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e07ec46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data,contingency_matrix):\n",
    "    \n",
    "    n_total = data.shape[0]\n",
    "    gt_classes=contingency_matrix.shape[0]\n",
    "    predicted_classes=contingency_matrix.shape[1]\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    # True Positive \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if contingency_matrix[i][j] != 1 and contingency_matrix[i][j] != 0:\n",
    "                TP += math.comb(int(contingency_matrix[i][j]),2)\n",
    "\n",
    "    # True Negative \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if i != j:\n",
    "                for k in range(predicted_classes):\n",
    "                    temp = contingency_matrix[k,i]*(np.sum(contingency_matrix[:,j]) - contingency_matrix[k,j])\n",
    "                    TN += temp\n",
    "    TN = TN/2\n",
    "\n",
    "    # False Positive \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            temp = contingency_matrix[j,i]*(np.sum(contingency_matrix[:,i])-contingency_matrix[j,i])/2\n",
    "            FP += temp\n",
    "\n",
    "    # False Negative \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if i != j:\n",
    "                for k in range(predicted_classes):\n",
    "                    temp = contingency_matrix[k,i]*(contingency_matrix[k,j])\n",
    "                    FN += temp\n",
    "    FN /= 2\n",
    "\n",
    "    # Jaccard Index\n",
    "    jacc = TP / (TP + FN + FP)\n",
    "\n",
    "    # Rand Index\n",
    "    rand = (TP + TN)/ (TP + FN + FP + TN)\n",
    "    print('---------Confusion Matrix----------')\n",
    "    print(f\"Rand Index: {rand}\")\n",
    "    \n",
    "    print(f\"Jaccard Index: {jacc}\")\n",
    "    print(f'TP= {TP},TN= {TN},FN= {FN},FP= {FP}')\n",
    "    \n",
    "    ht_c = 0\n",
    "    for i in range(gt_classes):\n",
    "        cluster_elem = np.sum(contingency_matrix[:,i])\n",
    "        for j in range(predicted_classes):  \n",
    "            temp = contingency_matrix[j][i]/cluster_elem\n",
    "            if temp != 0:\n",
    "                ht_c += temp*math.log(temp,2)*cluster_elem/n_total\n",
    "    ht_c = -1*ht_c\n",
    "    print('---------Conditional Entropy----------')\n",
    "    print(f\"Conditional Entropy: {ht_c}\")\n",
    "    \n",
    "    print('----------------Purity----------------')\n",
    "    purity=0\n",
    "    purities=[]\n",
    "    recalls=[]\n",
    "    for i in range(predicted_classes):\n",
    "        cluster_sum=np.sum(contingency_matrix[:,i])\n",
    "        class_max=np.max(contingency_matrix[:,i])\n",
    "        a=contingency_matrix[:,i]\n",
    "        max_index=a.argmax()\n",
    "        purities.append(class_max/cluster_sum)\n",
    "        recalls.append(class_max/np.sum(contingency_matrix[max_index,:]))\n",
    "        purity+=(class_max/cluster_sum) * (cluster_sum/n_total)\n",
    "    #purity = np.sum(np.max(contingency_matrix, axis =0))/np.sum(contingency_matrix)\n",
    "    print(f\"Purity: {purity}\")\n",
    "    #print(\"Purities\",purities)\n",
    "    \n",
    "    print('--------------F-measure---------------')\n",
    "    # a row for each cluster, and columns are precision, recall and F-measure respectively\n",
    "    \n",
    "    f_measure=0\n",
    "    for i in range(predicted_classes):\n",
    "        f_measure+=(2*purities[i]*recalls[i])/(purities[i]+recalls[i])\n",
    "    f_measure=f_measure/predicted_classes\n",
    "    print(f\"F: {f_measure}\")\n",
    "    \n",
    "    print('--------------Max matching---------------')\n",
    "    row_ind, col_ind = linear_sum_assignment(contingency_matrix, maximize=True)\n",
    "    contingency_reordered = contingency_matrix[row_ind][:, col_ind]\n",
    "    #print(contingency_reordered)\n",
    "    max_match = np.sum(np.diag(contingency_reordered))/np.sum(contingency_matrix)\n",
    "    print(f\"Max Matching: {max_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd16517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 23\n",
    "num_elements = len(labels)\n",
    "contingency_matrix = np.zeros((num_classes,num_classes))\n",
    "for i in range(num_elements):\n",
    "    contingency_matrix[y_train[i],labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f98365bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Confusion Matrix----------\n",
      "Rand Index: 0.4672537036309752\n",
      "Jaccard Index: 0.16607228898941917\n",
      "TP= 1531700,TN= 5214159.0,FN= 7453691.0,FP= 237701.0\n",
      "---------Conditional Entropy----------\n",
      "Conditional Entropy: 0.389318100386387\n",
      "----------------Purity----------------\n",
      "Purity: 0.8823967249720878\n",
      "--------------F-measure---------------\n",
      "F: 0.27426062013312574\n",
      "--------------Max matching---------------\n",
      "Max Matching: 0.2967994045403796\n"
     ]
    }
   ],
   "source": [
    "evaluation(X_train,contingency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4b8fe",
   "metadata": {},
   "source": [
    "# Testing K-means using Test Data set and Mapping Clusters to Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "502644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_spectral, labels_spectral, test_size=0.995, train_size=0.005,stratify=labels_spectral,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "65a52ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=23, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "96cd5808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=23, random_state=42)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "26774a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3b402396",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 11, 1: 9, 2: 9, 3: 11, 4: 11, 5: 20, 6: 11, 7: 11, 8: 11, 9: 11, 10: 11, 11: 11, 12: 11, 13: 11, 14: 11, 15: 11, 16: 11, 17: 17, 18: 5, 19: 11, 20: 11, 21: 11, 22: 11}\n"
     ]
    }
   ],
   "source": [
    "mapping, train_labels = map_and_change(y_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c43a6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5091199",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = map_and_change_test(mapping,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e1aa6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(test_labels, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e7435e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9860987754506749\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
