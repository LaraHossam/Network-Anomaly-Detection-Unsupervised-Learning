{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8ac85e",
   "metadata": {},
   "source": [
    "# Network Anomaly Detection using Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72b7b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import random\n",
    "random.seed(42)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b614b6f",
   "metadata": {},
   "source": [
    "# 1. Importing Data and Understanding Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ccd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.data_preprocessing import preprocess_data_10, preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce29219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k_means, labels_kmeans = preprocess_data_10()\n",
    "data_spectral, labels_spectral = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0051450",
   "metadata": {},
   "source": [
    "# 2.  Clustering Using K-Means and Normalized Cut (Your implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ab066",
   "metadata": {},
   "source": [
    "We are given a dataset of network traffic data represented as feature vectors of 41 dimensions. Our goal is to cluster this data using K-Means and identify any anomalies present in the data. \n",
    "\n",
    "The K-Means algorithm is a popular clustering algorithm that partitions the data into K clusters based on their similarities. It works by iteratively assigning each data point to its nearest cluster centroid and updating the centroids based on the newly assigned data points. This process continues until the centroids no longer move significantly.\n",
    "\n",
    "To perform K-Means clustering on the network traffic data, we will vary the value of K between 7, 15, 23, 31, and 45 clusters. This will produce different sets of clusters, allowing us to analyze the data at different levels of granularity. We will then evaluate the clusters to identify any anomalies present in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06553527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It takes two attrs k number of centroids and the whole data set number of samples x features\n",
    "\n",
    "def kMeans_implemented(k,data):\n",
    "    centroids=[]\n",
    "    num_points=data.shape[0]\n",
    "    num_features=data.shape[1]\n",
    "    \n",
    "    #Appending random points to be our centroids according to the number of ks\n",
    "    for i in range(k):\n",
    "        centroids.append(data[random.randint(0, num_points)])\n",
    "    clusters={}\n",
    "    t=0\n",
    "    while(True):\n",
    "        labels=[]\n",
    "        #Initialize empty clusters\n",
    "        for i in range (k):\n",
    "            clusters[i]=[]\n",
    "            \n",
    "        #Classify the points according to the closest centroid\n",
    "        for i in range(num_points):\n",
    "            distances=[]\n",
    "            for j in range(k):\n",
    "                distances.append(np.linalg.norm(data[i]-centroids[j]))\n",
    "            clusters[distances.index(min(distances))].append(data[i])\n",
    "            labels.append(distances.index(min(distances)))\n",
    "        new_centroids=np.zeros((k,num_features))\n",
    "        \n",
    "        #Measuring the new centroids\n",
    "        for i in range(k):\n",
    "            new_centroids[i]=np.mean(clusters[i],axis=0)\n",
    "        if(centroids==new_centroids).all():\n",
    "            break\n",
    "        else:\n",
    "            centroids=new_centroids\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ffd81",
   "metadata": {},
   "source": [
    "# description missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "513351b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def spectral_clustering(A,k):\n",
    "        \n",
    "    #--------------computing the degree matrix-------------\n",
    "    d = np.diag(np.sum(A, axis=1))\n",
    "\n",
    "    #--------------------computing L-----------------------\n",
    "    L = d-A\n",
    "\n",
    "    #---------------------computing La---------------------\n",
    "    #computing the inverse of the dgree matrix\n",
    "    inv_degree = np.linalg.inv(d)\n",
    "    La = np.dot(inv_degree, L)\n",
    "\n",
    "    #---computing the eigenValues and eigenVectors of La---\n",
    "    e_val, evec = np.linalg.eig(La)\n",
    "\n",
    "    #----------sorting the eigenValues ascending----------- \n",
    "    idx = np.argsort(eval)\n",
    "    e_val = e_val[idx]\n",
    "\n",
    "    #---sorting the eigenVectors according to their corresponding eigenValues---\n",
    "    evec = evec[:, idx]\n",
    "\n",
    "    #--slicing the eigenVectors to the desired number of clusters--\n",
    "    evec_new = evec[:, :k]\n",
    "\n",
    "    #-------------normalizing the eigenVectors--------------\n",
    "    system = evec.real / np.sqrt(np.linalg.norm(evec.real))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    system_labels = kmeans.fit_predict(system)\n",
    "\n",
    "\n",
    "    return system, system_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35f14e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_spectral, labels_spectral, test_size=0.995, train_size=0.005,stratify=labels_spectral,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e18a506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix=rbf_kernel(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da334a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "system,labels=spectral_clustering(sim_matrix,23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c36e2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35ea5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data,contingency_matrix):\n",
    "    \n",
    "    n_total = data.shape[0]\n",
    "    gt_classes=contingency_matrix.shape[0]\n",
    "    predicted_classes=contingency_matrix.shape[1]\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    # True Positive \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if contingency_matrix[i][j] != 1 and contingency_matrix[i][j] != 0:\n",
    "                TP += math.comb(int(contingency_matrix[i][j]),2)\n",
    "\n",
    "    # True Negative \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if i != j:\n",
    "                for k in range(predicted_classes):\n",
    "                    temp = contingency_matrix[k,i]*(np.sum(contingency_matrix[:,j]) - contingency_matrix[k,j])\n",
    "                    TN += temp\n",
    "    TN = TN/2\n",
    "\n",
    "    # False Positive \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            temp = contingency_matrix[j,i]*(np.sum(contingency_matrix[:,i])-contingency_matrix[j,i])/2\n",
    "            FP += temp\n",
    "\n",
    "    # False Negative \n",
    "    for i in range(gt_classes):\n",
    "        for j in range(predicted_classes):\n",
    "            if i != j:\n",
    "                for k in range(predicted_classes):\n",
    "                    temp = contingency_matrix[k,i]*(contingency_matrix[k,j])\n",
    "                    FN += temp\n",
    "    FN /= 2\n",
    "\n",
    "    # Jaccard Index\n",
    "    jacc = TP / (TP + FN + FP)\n",
    "\n",
    "    # Rand Index\n",
    "    rand = (TP + TN)/ (TP + FN + FP + TN)\n",
    "    print('---------Confusion Matrix----------')\n",
    "    print(f\"Rand Index: {rand}\")\n",
    "    \n",
    "    print(f\"Jaccard Index: {jacc}\")\n",
    "    print(f'TP= {TP},TN= {TN},FN= {FN},FP= {FP}')\n",
    "    \n",
    "    ht_c = 0\n",
    "    for i in range(gt_classes):\n",
    "        cluster_elem = np.sum(contingency_matrix[:,i])\n",
    "        for j in range(predicted_classes):  \n",
    "            temp = contingency_matrix[j][i]/cluster_elem\n",
    "            if temp != 0:\n",
    "                ht_c += temp*math.log(temp,2)*cluster_elem/n_total\n",
    "    ht_c = -1*ht_c\n",
    "    print('---------Conditional Entropy----------')\n",
    "    print(f\"Conditional Entropy: {ht_c}\")\n",
    "    \n",
    "    print('----------------Purity----------------')\n",
    "    purity=0\n",
    "    purities=[]\n",
    "    recalls=[]\n",
    "    for i in range(predicted_classes):\n",
    "        cluster_sum=np.sum(contingency_matrix[:,i])\n",
    "        class_max=np.max(contingency_matrix[:,i])\n",
    "        a=contingency_matrix[:,i]\n",
    "        max_index=a.argmax()\n",
    "        purities.append(class_max/cluster_sum)\n",
    "        recalls.append(class_max/np.sum(contingency_matrix[max_index,:]))\n",
    "        purity+=(class_max/cluster_sum) * (cluster_sum/n_total)\n",
    "    #purity = np.sum(np.max(contingency_matrix, axis =0))/np.sum(contingency_matrix)\n",
    "    print(f\"Purity: {purity}\")\n",
    "    print(\"Purities\",purities)\n",
    "    \n",
    "    print('--------------F-measure---------------')\n",
    "    # a row for each cluster, and columns are precision, recall and F-measure respectively\n",
    "    \n",
    "    f_measure=0\n",
    "    for i in range(predicted_classes):\n",
    "        f_measure+=(2*purities[i]*recalls[i])/(purities[i]+recalls[i])\n",
    "    f_measure=f_measure/predicted_classes\n",
    "    print(f\"F: {f_measure}\")\n",
    "    \n",
    "    print('--------------Max matching---------------')\n",
    "    row_ind, col_ind = linear_sum_assignment(contingency_matrix, maximize=True)\n",
    "    contingency_reordered = contingency_matrix[row_ind][:, col_ind]\n",
    "    #print(contingency_reordered)\n",
    "    max_match = np.sum(np.diag(contingency_reordered))/np.sum(contingency_matrix)\n",
    "    print(f\"Max Matching: {max_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "716af55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 23\n",
    "num_elements = len(labels)\n",
    "contingency_matrix = np.zeros((num_classes,num_classes))\n",
    "for i in range(num_elements):\n",
    "    contingency_matrix[y_train[i],labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60e346cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Confusion Matrix----------\n",
      "Rand Index: 0.4672537036309752\n",
      "Jaccard Index: 0.16607228898941917\n",
      "TP= 1531700,TN= 5214159.0,FN= 7453691.0,FP= 237701.0\n",
      "---------Conditional Entropy----------\n",
      "Conditional Entropy: 0.389318100386387\n",
      "----------------Purity----------------\n",
      "Purity: 0.8823967249720878\n",
      "Purities [0.6153846153846154, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.76, 0.5086206896551724, 0.9956204379562044, 1.0, 0.9966923925027563, 0.7777777777777778, 0.9823321554770318, 0.6086956521739131, 0.7872340425531915, 0.8133498145859085, 0.875, 0.9649122807017544, 0.9829192546583851, 0.9987012987012988, 0.9565217391304348]\n",
      "--------------F-measure---------------\n",
      "F: 0.27426062013312574\n",
      "--------------Max matching---------------\n",
      "Max Matching: 0.2967994045403796\n"
     ]
    }
   ],
   "source": [
    "evaluation(X_train,contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931891f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
